---
layout: post
title: 信息论小记
category: tech
---

# 信息量（Shannon Information）
一个事件如事件发生的概率越大，表示该事件发生所包含的信息量越小，反之亦然。比如某事件发生的概率是100%，那么该事件的信息量为0。我们可以通过事件发生的概率计算事件发生所包含的的信息量。

对于离散变量$X$ ，我们可以用以下公式表示事件$X=x_0$的信息量：
$$\begin{align*}
I(x_0) = \log\frac {1 }{ p(x_0)} =  -\log p(x_0)
\end{align*}$$



# 信息熵（Information Entropy）
信息熵是信息论最基本的概念之一，用于衡量变量或信息的随机性（不确定性）。信息熵在信息论里的定义就是目标变量的信息量期望。

对于离散变量$X$ ，我们可以用以下公式表示$X$的信息熵：

$$\begin{align*}
H(X) = \mathbb{E}[\log(\frac 1 {p(X)})]= -\mathbb{E}[\log(p(X))] =  -\sum_{x\in X} p(x) \log p(x)
\end{align*}$$



# 联合熵（Joint Entropy）
联合熵用于衡量两个随机变量联合分布的不确定性。对于变量$X$和$Y$的联合熵，我们可以用以下公式表示：

$$\begin{align*}
H(X,Y) =  -\mathbb{E}[\log p(X,Y)] = -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log p(x,y)
\end{align*}$$



**联合熵的一些重要性质：**
 
$$H(X,X) = H(X) + H(X)$$

$$H(X,Y) = H(Y,X)$$

当$X$和$Y$相互独立时，$$H(X,Y) = H(X) + H(Y)$$



# 条件熵（Conditional Entropy）
条件熵用于衡量随机变量$X$的条件下，随机变量$Y$的不确定性。我们可以用以下公式表示条件熵：

$$
\begin{align*}
    {H}(Y \mid X) 
    &=\sum_{x \in X} p(x)H(Y\mid X=x)  \\
    &= -\sum_{x \in X} p(x)\sum_{y \in Y} p(y\mid x) \log p(y \mid x)\\
    &= -\sum_{x \in X} p(x)\sum_{y \in Y} p(y\mid x) \log \frac{p\left(x, y\right)}{p\left(x\right)}\\
    &= -\sum_{x \in X}\sum_{y \in Y} p\left(x, y\right) \log p(y\mid x)\\

\end{align*}
$$

**条件熵的一些重要性质：**
 
$$H(X \mid Y) \neq H(Y \mid X)$$



# 互信息（Mutual Information）
互信息是用于衡量两个随机变量之间相关性的度量。它是一个非负的度量，并且当且仅当两个随机变量相互独立时，互信息为0。

我们可以用以下公式表示互信息：

$$
I(X;Y) = H(X)-H(X\mid Y) = H(Y)- H(Y\mid X)
$$

**互信息的一些重要性质：**

$$I(X;X) = H(X)$$

$$I(X;Y) = H(X) + H(Y) - I(X;Y)$$

当且仅当X,Y相互独立时，$$I(X;Y) = 0$$


# 链式法则（Chain Rule）
上述的信息熵，联合熵，条件熵的关系，符合信息论中常用的链式法则：

$$ H(X \mid Y) + H(Y) = H(Y\mid X) + H(X) = H(X,Y) $$

上述各种信息熵，及互信息之间满足以下关系：






# KL散度（Kullback-Leibler Divergence）
KL散度在信息量中又称为相对熵（Relative Entropy），是信息论中常用的衡量两个概率分布差异的度量。

假设$P(x)$, $Q(x)$是变量$X$取值的两个概率分布，则$P$对$Q$的相对熵是：

$$
\begin{align*}
D_{\mathrm{KL}}(P \| Q)
&:= - \sum_{x \in X} P(x)\left[\log \left(Q(x)\right)  - \log\left( P(x)\right) \right] \\
&=  \sum_{x \in X} P(x) \log \left(\frac{P(x)}{Q(x)}\right)
\end{align*}
$$

**KL散度的一些重要性质：**

KL散度总是非负的

通常来说，KL散度是不对称，因此他并不是一个距离度量，也不满足三角不等式。

通常来说KL散度没有上界

当分布P和分布Q越接近，KL散度越小。



# 交叉熵（Cross Entropy）